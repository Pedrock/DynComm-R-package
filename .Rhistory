aggregate(data= get, x=get$text, by=get$user_id, FUN=function(x)paste(x, collapse = " "))
get$user_id
aggregate(data= get, x=get$text, by=list(unique(get$user_id)), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, x=get$text, by=list(unique(get$user_id)), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, x=get$text, by=unique(get$user_id), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, x=get$text, by=list(user_id), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, x="text", by=list(c("user_id")), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, x="text", by=list(c("user_id")), FUN=function(x)paste(x, collapse = " "))
aggregate(data= get, formula = text ~ user_id, FUN=function(x)paste(x, collapse = " "))
aggregate(x= get, by=list(factor(user_id)), FUN=function(x)paste(x, collapse = " "))
aggregate(x= get, by=list(factor(get$user_id)), FUN=function(x)paste(x, collapse = " "))
aggregate(data=get , x=get$text, by=list(factor(get$user_id)), FUN=function(x)paste(x, collapse = " "))
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
all.text.user
all.text.user[1:10,1:10]
all.text.user[1:10]
all.text.user[1:10,]
all.text.user[1:2,]
all.text.user[1:2,]
all.text.user <- aggregate(text ~ user_id + user_name, data = get, paste, collapse = " ")
all.text.user <- aggregate(text ~ user_id + username, data = get, paste, collapse = " ")
all.text.user <- aggregate(text ~ user_id + user_name, data = get, paste, collapse = " ")
names(get)
all.text.user <- aggregate(text ~ user_id + name, data = get, paste, collapse = " ")
browser()
all.text.user[1:2,]
corp<-makeCorpus(all.text.user) #creating corpus
traceback()
library(lsa)
myReader <- readTabular(mapping=list(content="text", id="name"))
??readTabular
?readerControl
??readerControl
?Corpus
library(tm)
?Corpus
?readerControl
?mapping
??mapping
?DataframeSource
corpus <- Corpus(DataframeSource(all.text.user,mapping=list(content="text", id="name")))
corpus <- Corpus(DataframeSource(all.text.user))
all.text.user <- data.frame(doc_id = all.text.user$name, text=all.text.user$text, stringsAsFactors = FALSE)
all.text.user <- aggregate(text ~ user_id + name, data = get, paste, collapse = " ")
?DocumentTermMatrix
??DocumentTermMatrix
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
corpus
dtm.text <- DocumentTermMatrix(corpus, control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
stopwords = TRUE))
dtm.text
dtm.text <- DocumentTermMatrix(corpus, control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
stopwords = FALSE))
dtm.text <- DocumentTermMatrix(corpus, control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = FALSE))
dtm.text <- DocumentTermMatrix(corpus, control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE))
dtm.text <- DocumentTermMatrix(corpus, control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
tolower = TRUE))
all.text.user <- data.frame(doc_id = all.text.user$name, text=iconv(all.text.user$text, "ASCII", "UTF-8", sub="byte"), stringsAsFactors = FALSE)
corpus <- Corpus(DataframeSource(all.text.user))
#browser()
all.text.user <- aggregate(text ~ user_id + name, data = get, paste, collapse = " ")
all.text.user <- data.frame(doc_id = all.text.user$name, text=iconv(all.text.user$text, "ASCII", "UTF-8", sub="byte"), stringsAsFactors = FALSE)
corpus <- Corpus(DataframeSource(all.text.user))
#browser()
all.text.user <- aggregate(text ~ user_id + name, data = get, paste, collapse = " ")
iconv(all.text.user$text, "ASCII", "UTF-8", sub="byte")
hexToText <- function(msg){
hex <- sapply(seq(1, nchar(as.character(msg)), by=2),
function(x) substr(msg, x, x+1))
hex <- subset(hex, !hex == "00")
gsub('[^[:print:]]+', '', rawToChar(as.raw(strtoi(hex, 16L))))
}
all.text.user <- data.frame(doc_id = all.text.user$name, text=iconv(hexToText(all.text.user$text), "ASCII", "UTF-8", sub="byte"), stringsAsFactors = FALSE)
all.text.user <- data.frame(doc_id = all.text.user$name, text=iconv(hexToText(all.text.user$text), "ASCII", "UTF-8", sub="byte"), stringsAsFactors = FALSE)
hexToText(all.text.user$text)
hexToText(all.text.user$text[1])
iconv(all.text.user$text, "ASCII", "UTF-8", sub="byte")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
traceback()
#global word count
freqPlot(paste(all.text.user$text, collapse = " ")) #creating frequency plots
paste(all.text.user$text, collapse = " ")
#global word count
freqPlot(all.text.user$text) #creating frequency plots
#global word count
freqPlot(list(all.text.user$text)) #creating frequency plots
#global word count
freqPlot(list(all.text.user$text)) #creating frequency plots
# Pre-processing of Corpus
makeCorpus <- function(text){ #Function for making corpus and cleaning the tweets fetched
#twitterdf <- do.call("rbind", lapply(text, as.data.frame)) #store the fetched tweets as a data frame
twitterdf <- data.frame(text=text, stringsAsFactors = FALSE)
twitterdf$text <- sapply(twitterdf$text,function(row) iconv(row, "latin1", "ASCII", sub=""))#Removing emoticons from tweets
twitterCorpus <- Corpus(VectorSource(twitterdf$text)) #Creating Corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) #function to replace a pattern to white space using regex
twitterCorpus <- tm_map(twitterCorpus, toSpace, "(RT|via)((?:\\b\\W*@\\w+)+)") #match rt or via
twitterCorpus <- tm_map(twitterCorpus, toSpace, "@\\w+") #match @
twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ \t]{2,}") #match tabs
twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ |\n]{1,}") #match new lines
twitterCorpus <- tm_map(twitterCorpus, toSpace, "^ ") #match white space at begenning
twitterCorpus <- tm_map(twitterCorpus, toSpace, " $") #match white space at the end
twitterCorpus <- tm_map(twitterCorpus, PlainTextDocument)
twitterCorpus <- tm_map(twitterCorpus, removeNumbers)
twitterCorpus <- tm_map(twitterCorpus, removePunctuation)
twitterCorpus <- tm_map(twitterCorpus, toSpace, "http[[:alnum:]]*") #remove url from tweets
twitterCorpus <- tm_map(twitterCorpus,removeWords,stopwords("en"))
twitterCorpus <- tm_map(twitterCorpus, content_transformer(tolower))
return(twitterCorpus)
}
#global word count
freqPlot(all.text.user$text) #creating frequency plots
#global word cloud
makeWordcloud(all.text.user$text) #creating wordcloud
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster(costable.dtm.text) #hierarchical clustering
costable.dtm.text
?hcluster
??hcluster
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hClust(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster(costable.dtm.text) #hierarchical clustering
traceback()
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#Clustering
hCluster.users<-function (sim.matrix){ #hierarchical clustering
fit <- hclust(sim.matrix, method="ward.D") #clustering terms
plot(fit)
rect.hclust(fit, k=5) #cutting the tree into 5 clusters
(groups <- cutree(fit, k=5))
}
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(as.matrix(costable.dtm.text)) #hierarchical clustering
traceb ack()
traceback()
hclust(costable.dtm.text, method="ward.D")
?hclust
hclust(as.dist(costable.dtm.text), method="ward.D")
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(as.dist(costable.dtm.text)) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
names(hCluster.users(as.dist(costable.dtm.text))) #hierarchical clustering
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
fit
fit
names(fit)
fit$order
fit$labels
fit$height
fit$merge
fit
groups
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
?dialogWindow
??dialogWindow
??dialog
shiny::runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
traceback()
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
clust.groups
i=1
names(clust.groups[which(clust.groups==i)])
users.clust <- names(clust.groups[which(clust.groups==i)])
paste(all.text.user[which(all.text.user$doc_id %in% users.clust),"text"])
text.clust <- paste(all.text.user[which(all.text.user$doc_id %in% users.clust),"text"])
tSentimen(text.clust)
tSentimen(paste(text.clust, collapse=" "))
paste(text.clust, collapse=" ")
makeCorpus(text.clust)
unlist(sapply(twicorpus, `[`, "content"))
twicorpus<-makeCorpus(text.clust)
unlist(sapply(twicorpus, `[`, "content"))
names(twicorpus)
twicorpus[[1]]
twicorpus[[1]]$content
twicorpus[[]]$content
twicorpus[[1:2]]$content
twicorpus[[1]]$content
twicorpus[[2]]$content
unlist(twicorpus[[]])$content
unlist(twicorpus)$content
unlist(twicorpus)
class(unlist(twicorpus))
lapply(twicorpus, FUN=function(x){x$content})
lapply(twicorpus, FUN=function(x){x$"content"})
?lapply
twicorpus$content
data.frame(text=twicorpus$content, stringsAsFactors=F)
nrows(data.frame(text=twicorpus$content, stringsAsFactors=F))
nrow(data.frame(text=twicorpus$content, stringsAsFactors=F))
ncol(data.frame(text=twicorpus$content, stringsAsFactors=F))
dataframe<-data.frame(text=twicorpus$content, stringsAsFactors=F)
poldat <- with(dataframe, polarity(text))
names(dataframe)
poldat <- with(dataframe, polarity("text"))
poldat <- with(dataframe, polarity(paste(twicorpus$content,collapse = " ")))
poldat <- polarity(paste(twicorpus$content,collapse = " "))
polarity(paste(twicorpus$content,collapse = " "))
traceback()
`[[.qdap_hash` <- `[[.data.frame`
poldat <- with(dataframe, polarity(text))
tSentimen(text.clust)
text.clust
tSentimen(text.clust)$stan.mean.polarit
tSentimen(text.clust)$stan.mean.polarity
tSentimen(text.clust)["stan.mean.polarity"]
names(tSentimen(text.clust))
tSentimen(text.clust)$all
tSentimen(text.clust)$group
tSentimen(text.clust)$group$stan.mean.polarity
?polarity
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
??rtweet
# Set up authentication
Auth<-create_token(consumer_key=APIKey, consumer_secret=APISecret,access_token = AccessToken, access_secret =AccessTokenSecret)
Auth
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
remove.packages("twitteR")
library(twitteR)
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
remove.packages(qdap)
remove.packages("qdap")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
install.packages("qdap")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Finance-Twitter-News-Sentiment-Per-Users-Groups.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("quantmod")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("tseries")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("caret")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("ipred")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("rio")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("tibble")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("purrr")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("colorspace")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("data.table")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("xgboost")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("ps")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
url
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
anova(lasso_mod, xgb_mod)
anova(xgb_mod, model)
k = ncol(X_train)
## create your model,and add layers
model <- keras_model_sequential()
model %>%
layer_dense(units = 60, activation = 'relu', input_shape = k) %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 1, activation = 'linear')
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = 'mse'
)
###########################
# Step 2: Train the model #
###########################
model %>% fit(X_train, y_train, epochs=100, batch_size=28, validation_split = 0.1)
################################
# Step 2: Plot the predictions #
################################
pred <- model %>% predict(X_test, batch_size = 28)
print("Next Seven Days Forecast using Neural Network Regression will be : ")
print(pred)
#################################
# Compare the Models with Anova #
#################################
anova(lasso_mod, xgb_mod)
anova(xgb_mod, model)
?anova
model
xgb_mod
lasso_mod
class(lasso_mod)
class(xgb_mod)
class(model)
library(rPython)
install.packages("rPython")
install.packages("Rtools")
install.packages("devtools")
install.packages("Rtools")
library(Rtools)
library(Rtools)
library(Rtools)
library(devtools)
library(rPython)
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
?python.load
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
?py_install
?source_python
inf
?inf
infinity
1/0
Inf
import("networkx")
?nx.Graph()
Caca <- 2
caca <- 3
Caca
caca
shiny::runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
?as.table
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
shiny::runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
?plot
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
plot(trends.table[1:nrow(trends.table),])
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
plot(trends.table[1:nrow(trends.table),])
trends.table
plot(trends.table)
plot(trends.table[1,])
cite(IncRcpa)
cite(Rcpa)
cite(incRcpa)
cite(onlinePCA)
cite("onlinePCA")
library("onlinePCA")
cite(onlinePCA)
cite("onlinePCA")
library("onlinePCA")
cite(onlinePCA)
cite("onlinePCA")
library(gtrendsR)
cite(gtrendsR)
cite("gtrendsR")
shiny::runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
View(trends.table)
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
keys.list[ngd.i]
input$Region
NGD
list
results <- list(NGD=NGD,
x=c(x, freq.locality),
y=c(x, freq.capital),
xy=c(paste("Local + Capital"), freq.loccap))
results
as.data.frame(results)
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
keys
key
sub_code
input$Region
local
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
length(gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits)
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
?tabPanel
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
ngd
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity/CZECH-DATA-FILTER.R')
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity")
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity/CZECH-DATA-FILTER.R')
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity")
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity/CZECH-DATA-FILTER.R')
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity")
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity/CZECH-DATA-FILTER.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity/CZECH-DATA-FILTER.R')
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Bipartite Incremental Similarity")
?getForm
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R")
setwd("C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/")
library(devtools)
document()
install.packages("rPython")
document()
install("C:/Users/carlos/Downloads/rPython")
install("C:/Users/Rui Sarmento/Downloads/rPython")
install("C:/Users/Rui Sarmento/Downloads/rPython")
setwd("~/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN")
setwd("~/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN")
setwd("~/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package")
setwd("~/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN")
library(devtools)
document()
shiny::runApp('~/Dropbox/Projectos/Idealize Shiny/idealize/Shiny')
setwd("~/Dropbox/Projectos/Idealize Shiny/idealize/Shiny")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
